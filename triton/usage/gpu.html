

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>GPU Computing &mdash; Aalto scientific computing</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/theme_overrides.css" type="text/css" />
  

  
    <link rel="top" title="Aalto scientific computing" href="../../index.html"/>
        <link rel="up" title="Triton user guide" href="../index.html"/>
        <link rel="next" title="Grid computing" href="grid.html"/>
        <link rel="prev" title="Running programs on Triton" href="general.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Aalto scientific computing
          

          
            
            <img src="../../_static/aalto.png" class="logo" />
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/welcomeresearchers.html">Welcome, researchers!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/welcomestudents.html">Welcome, students!</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../news/index.html">News</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/index.html">The Aalto environment</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../data/index.html">Data</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Triton user guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#quick-contents-and-links">Quick contents and links</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials">Tutorials</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#detailed-instructions">Detailed instructions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="compilers.html">Available compilers</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="dgx.html">Nvidia DGX machines</a></li>
<li class="toctree-l3"><a class="reference internal" href="faq.html">Frequently asked questions</a></li>
<li class="toctree-l3"><a class="reference internal" href="general.html">Running programs on Triton</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">GPU Computing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hardware-breakdown">Hardware breakdown</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-gpu-nodes">Using GPU nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-gpu-usage">Monitoring GPU usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#development">Development</a></li>
<li class="toctree-l4"><a class="reference internal" href="#applications-and-known-issues">Applications and known issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attachments-and-useful-links">Attachments and useful links</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="grid.html">Grid computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="grid2.html">Grid computing 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="jobs.html">Monitoring jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="libs.html">Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="localstorage.html">Storage: local drives</a></li>
<li class="toctree-l3"><a class="reference internal" href="lustre.html">Storage: Lustre (scratch)</a></li>
<li class="toctree-l3"><a class="reference internal" href="mpilibs.html">MPI on Triton</a></li>
<li class="toctree-l3"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="quotas.html">Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="singularity.html">Singularity Containers</a></li>
<li class="toctree-l3"><a class="reference internal" href="smallfiles.html">Small files</a></li>
<li class="toctree-l3"><a class="reference internal" href="storage.html">Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="toolchains.html">Compilers and toolchains</a></li>
<li class="toctree-l3"><a class="reference internal" href="workflows.html">Remote workflows at Aalto</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#reference-and-examples">Reference and Examples</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../scicomp/index.html">Scientific computing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Training</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">About these docs</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Aalto scientific computing</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">Triton user guide</a> &raquo;</li>
      
    <li>GPU Computing</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="https://github.com/AaltoScienceIT/scicomp-docs/blob/master/triton/usage/gpu.rst" class="fa fa-github"> Edit on GitHub</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gpu-computing">
<h1>GPU Computing<a class="headerlink" href="#gpu-computing" title="Permalink to this headline">¶</a></h1>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">Introductory tutorial: <a class="reference internal" href="../tut/gpu.html"><em>GPU computing</em></a> (read this first)</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Triton has GPU cards from four different NVIDIA generations, as
described below.</p>
</div>
<div class="section" id="hardware-breakdown">
<h2>Hardware breakdown<a class="headerlink" href="#hardware-breakdown" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Card</td>
<td>total amount</td>
<td>nodes</td>
<td>architecture</td>
<td>compute threads per GPU</td>
<td>memory per card</td>
<td>CUDA compute capability</td>
<td>Slurm feature name</td>
<td>Slurm gres name</td>
</tr>
<tr class="row-even"><td>Tesla K80*</td>
<td>12</td>
<td>gpu[20-22]</td>
<td>Kepler</td>
<td>2x2496</td>
<td>2x12GB</td>
<td>3.7</td>
<td><code class="docutils literal"><span class="pre">kepler</span></code></td>
<td><code class="docutils literal"><span class="pre">teslak80</span></code></td>
</tr>
<tr class="row-odd"><td>Tesla P100</td>
<td>20</td>
<td>gpu[23-27]</td>
<td>Pascal</td>
<td>3854</td>
<td>16GB</td>
<td>6.0</td>
<td><code class="docutils literal"><span class="pre">pascal</span></code></td>
<td><code class="docutils literal"><span class="pre">teslap100</span></code></td>
</tr>
<tr class="row-even"><td>Tesla V100</td>
<td>40</td>
<td>gpu[28-37]</td>
<td>Volta</td>
<td>5120</td>
<td>32GB</td>
<td>7.0</td>
<td><code class="docutils literal"><span class="pre">volta</span></code></td>
<td><code class="docutils literal"><span class="pre">v100</span></code></td>
</tr>
<tr class="row-odd"><td>Tesla V100</td>
<td>16</td>
<td>dgx[01-02]</td>
<td>Volta</td>
<td>5120</td>
<td>16GB</td>
<td>7.0</td>
<td><code class="docutils literal"><span class="pre">volta</span></code></td>
<td><code class="docutils literal"><span class="pre">v100</span></code></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>Note: Tesla K80 cards are in essence two GK210 GPUs on a single chip</li>
<li>Note: V100 cards are part of DGX machines, which were purchased by
several groups and are currently special access only.  They are also a
different operating system, please see <a class="reference internal" href="dgx.html"><em>Nvidia DGX machines</em></a>.</li>
</ul>
<p>Detail info about cards available at
<a class="reference external" href="https://en.wikipedia.org/wiki/Nvidia_Tesla">https://en.wikipedia.org/wiki/Nvidia_Tesla</a> and for general info about
Nvidia GPUs
<a class="reference external" href="https://www.nvidia.com/object/tesla-supercomputing-solutions.html">https://www.nvidia.com/object/tesla-supercomputing-solutions.html</a></p>
</div>
<div class="section" id="using-gpu-nodes">
<h2>Using GPU nodes<a class="headerlink" href="#using-gpu-nodes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="gpu-partitions">
<h3>GPU partitions<a class="headerlink" href="#gpu-partitions" title="Permalink to this headline">¶</a></h3>
<p>There are two queues governing these nodes: <code class="docutils literal"><span class="pre">gpu</span></code> and <code class="docutils literal"><span class="pre">gpushort</span></code>, where the
latter is for jobs up to 4 hours.  Partitions are automatically selected.</p>
<p>The latest details can always be found with the following command:</p>
<div class="highlight-python"><div class="highlight"><pre>$ slurm p | grep gpu
</pre></div>
</div>
</div>
<div class="section" id="gpu-node-allocation">
<h3>GPU node allocation<a class="headerlink" href="#gpu-node-allocation" title="Permalink to this headline">¶</a></h3>
<p>For gpu resource allocation one has to request a GPU resource, with
<code class="docutils literal"><span class="pre">--gres=gpu:N</span></code> , where <code class="docutils literal"><span class="pre">N</span></code>
stands for number of requested GPU cards.  To request a GPU with a
certain architecture, use <code class="docutils literal"><span class="pre">--constraint=GENERATION</span></code>.   To request a specific card,
one must use syntax  <code class="docutils literal"><span class="pre">--gres=gpu:CARD_TYPE:N</span></code>.  See the table below
for &#8220;slurm feature name&#8221; or &#8220;Slurm gres name&#8221;.  For the full current
list of configured SLURM gpu cards names run <code class="docutils literal"><span class="pre">slurm</span> <span class="pre">features</span></code>.</p>
<p>Example usages:</p>
<div class="highlight-python"><div class="highlight"><pre>--gres=gpu:2
--gres=gpu:1 --constraint=pascal
--gres=gpu:telsap100:1
</pre></div>
</div>
<p>When using multiple GPU&#8217;s please verify that the code actually uses them with
instructions given in <a class="reference internal" href="#monitoring-gpu-usage">Monitoring GPU usage</a>.</p>
</div>
<div class="section" id="gpu-nodes-environment-and-cuda">
<h3>GPU nodes environment and CUDA<a class="headerlink" href="#gpu-nodes-environment-and-cuda" title="Permalink to this headline">¶</a></h3>
<p>User environment on <code class="docutils literal"><span class="pre">gpu*</span></code> nodes is the same as on other nodes, the
only difference is that they have nvidia kernel modules for Tesla cards.
<a class="reference external" href="https://www.nvidia.com/object/cuda_home_new.html">CUDA</a> comes through
<code class="docutils literal"><span class="pre">module</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre>$ module avail cuda    # list installed CUDA modules
$ module load cuda/10.0.130   # load CUDA environment that you need
$ nvcc --version   # see actual CUDA version that you got
</pre></div>
</div>
</div>
<div class="section" id="running-a-gpu-job-in-serial">
<h3>Running a GPU job in serial<a class="headerlink" href="#running-a-gpu-job-in-serial" title="Permalink to this headline">¶</a></h3>
<p>Quick interactive run:</p>
<div class="highlight-python"><div class="highlight"><pre>$ module load cuda
$ srun -t 00:30:00 --gres=gpu:1 $WRKDIR/my_gpu_binary
</pre></div>
</div>
<p>Allocating a gpu node for longer interactive session, this will give you
a shell sessions:</p>
<div class="highlight-python"><div class="highlight"><pre>$ module load cuda
$ sinteractive -t 4:00:00 --gres=gpu:1
gpuXX$ .... run something
gpuXX$ exit
</pre></div>
</div>
<p>Run a batch job</p>
<div class="highlight-python"><div class="highlight"><pre>$ sbatch gpu_job.sh
</pre></div>
</div>
<p>Where <code class="docutils literal"><span class="pre">gpu_job.sh</span></code> is</p>
<div class="highlight-python"><div class="highlight"><pre>#!/bin/bash

#SBATCH --time=01:15:00          ## wallclock time hh:mm:ss
#SBATCH --gres=gpu:teslak80:1    ## one K80 requested

module load cuda

## run my GPU accelerated executable, note the --gres
srun --gres=gpu:1  $WRKDIR/my_gpu_binary
</pre></div>
</div>
</div>
</div>
<div class="section" id="monitoring-gpu-usage">
<h2>Monitoring GPU usage<a class="headerlink" href="#monitoring-gpu-usage" title="Permalink to this headline">¶</a></h2>
<p>Currently there isn&#8217;t a good way of monitoring the gpu usage
non-interactively. Interactively one can (when the job is running) ssh to the
gpu node in question and run</p>
<div class="highlight-python"><div class="highlight"><pre>login2$ ssh gpuxx
gpuxx$ watch -n 1 nvidia-smi
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">CTRL</span> <span class="pre">+</span> <span class="pre">C</span></code> quits the command.</p>
<p>This shows the gpu usage with 1 second interval. The GPU utilized by process
with PID X is shown in the first column of the second table. The first table
lists the GPUs by their ID Checking the <code class="docutils literal"><span class="pre">Volatile</span> <span class="pre">GPU-Util</span></code> column gives the
utilization of GPU. If your code uses less than 50% of the GPU you should
try to improve the data loading / CPU part of your code as the GPU is
underutilized.</p>
<p>If you run multi-GPU job you should verify that the all GPUs are properly
utilized. For many applications one needs to use multiple CPUs to fill the
GPUs with data. With badly implemented data handling multi-GPU setups can
be slower than single-GPU setups.</p>
</div>
<div class="section" id="development">
<h2>Development<a class="headerlink" href="#development" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compiling">
<h3>Compiling<a class="headerlink" href="#compiling" title="Permalink to this headline">¶</a></h3>
<p>In case you either want to compile a CUDA code or a code with GPU
support, you must do it on one of the gpu nodes (because of nvidia libs
installed on those nodes only).</p>
<div class="highlight-python"><div class="highlight"><pre>$ sinteractive -t 1:00:00 --gres=gpu:1    # open a session on a gpu node
$ module load cuda                        # set CUDA environment
$ nvcc cuda_code.cu -o cuda_code          # compile your CUDA code
.. or compile normally any other code with &#39;make&#39;
</pre></div>
</div>
</div>
<div class="section" id="debugging">
<h3>Debugging<a class="headerlink" href="#debugging" title="Permalink to this headline">¶</a></h3>
<p>CUDA SDK provides an extension to the well-known gnu debugger gdb. Using
cuda-gdb it is possible to debug the device code natively on the GPU. In
order to use the <code class="docutils literal"><span class="pre">cuda-gdb</span></code>, one has to compile the program with option
pair <code class="docutils literal"><span class="pre">-g</span> <span class="pre">-G</span></code>, like follows:</p>
<div class="highlight-python"><div class="highlight"><pre>$ nvcc -g -G cuda_code.cu -o cuda_code
</pre></div>
</div>
<p>See <a class="reference external" href="https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/cuda-gdb.pdf">CUDA-GDB User
Guide</a>
for a more information on cuda-gdb.</p>
</div>
</div>
<div class="section" id="applications-and-known-issues">
<h2>Applications and known issues<a class="headerlink" href="#applications-and-known-issues" title="Permalink to this headline">¶</a></h2>
<p>Check the <a class="reference internal" href="../index.html#application-list"><span>Applications</span></a> for most software.</p>
<div class="section" id="nvidia-smi-utility">
<h3>nvidia-smi utility<a class="headerlink" href="#nvidia-smi-utility" title="Permalink to this headline">¶</a></h3>
<p>Could be useful for debugging, in case one want to see the actual gpu
cards available on the node. If this command returns an error, you should
report that something is wrong on the node.</p>
<div class="highlight-python"><div class="highlight"><pre>gpuxx$ nvidia-smi -L   # gives a list of GPU cards on the node
</pre></div>
</div>
</div>
<div class="section" id="cudnn">
<h3>cuDNN<a class="headerlink" href="#cudnn" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">cudnn</span></code> is available as a module. The latest version can be found with
<code class="docutils literal"><span class="pre">module</span> <span class="pre">spider</span> <span class="pre">cudnn</span></code>. Note that (at least the later versions of)
cudnn require newer cards and cannot be used on the old fermi cards.
E.g. tensorflow does not run on the older fermi cards for this reason.</p>
</div>
<div class="section" id="nvidia-mps">
<h3>Nvidia MPS<a class="headerlink" href="#nvidia-mps" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://docs.nvidia.com/deploy/mps/index.html">Nvidia Multi-Process Service (MPS)</a> provides a way to
share a single GPU among multiple processes. It can be used to
increase the GPU utilization by timesharing the GPU access, e.g. one
process can upload data to the GPU while another is running a
kernel. To use it one must first start the MPS server, and then CUDA
calls are automatically routed via the MPS server. At the end of the
job one must remember to shut it down. Example job script:</p>
<div class="highlight-python"><div class="highlight"><pre>#!/bin/bash -l

#SBATCH --time=01:15:00          ## wallclock time hh:mm:ss
#SBATCH --gres=gpu:teslak80:1    ## one K80 requested

module load cuda

## Start the MPS server
CUDA_MPS_LOG_DIRECTORY=nvidia-mps srun --gres=gpu:1 nvidia-cuda-mps-control -d&amp;

## run my GPU accelerated executable
srun --gres=gpu:1  $WRKDIR/my_gpu_binary

## Shut down the MPS server
echo &quot;quit&quot; | nvidia-cuda-mps-control
</pre></div>
</div>
</div>
<div class="section" id="cuda-samples">
<h3>CUDA samples<a class="headerlink" href="#cuda-samples" title="Permalink to this headline">¶</a></h3>
<p>There are CUDA code samples provided by Nvidia that can be useful for a
sake of testing or getting familiar with CUDA. Placed
at <code class="docutils literal"><span class="pre">$CUDA_ROOT/samples</span></code>. To play with:</p>
<div class="highlight-python"><div class="highlight"><pre>$ sinteractive -t 1:00:00 --gres=gpu:1
$ module load cuda
$ cp -r $CUDA_ROOT/samples $WRKDIR
$ cd $WRKDIR/samples
$ make TARGET_ARCH=x86_64
$ ./bin/x86_64/linux/release/deviceQuery
...
$ ./bin/x86_64/linux/release/bandwidthTest
...
</pre></div>
</div>
</div>
</div>
<div class="section" id="attachments-and-useful-links">
<h2>Attachments and useful links<a class="headerlink" href="#attachments-and-useful-links" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA_C_Programming_Guide.pdf">CUDA C Programming
Guide</a></li>
<li><a class="reference external" href="https://developer.nvidia.com/category/zone/cuda-zone">CUDA Zone on
NVIDIA</a></li>
<li><a class="reference external" href="https://developer.nvidia.com/cuda/cuda-faq">CUDA FAQ</a></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="grid.html" class="btn btn-neutral float-right" title="Grid computing" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="general.html" class="btn btn-neutral" title="Running programs on Triton" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Aalto Science-IT.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/redirect-to-https.js"></script>
      <script type="text/javascript" src="https://users.aalto.fi/~darstr1/minipres-stable.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>