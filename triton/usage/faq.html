

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Frequently asked questions &mdash; Aalto scientific computing</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/theme_overrides.css" type="text/css" />
  

  
    <link rel="top" title="Aalto scientific computing" href="../../index.html"/>
        <link rel="up" title="Triton user guide" href="../index.html"/>
        <link rel="next" title="Running programs on Triton" href="general.html"/>
        <link rel="prev" title="Nvidia DGX machines" href="dgx.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Aalto scientific computing
          

          
            
            <img src="../../_static/aalto.png" class="logo" />
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/welcomeresearchers.html">Welcome, researchers!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/welcomestudents.html">Welcome, students!</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../news/index.html">News</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../aalto/index.html">The Aalto environment</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../data/index.html">Data</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Triton user guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#quick-contents-and-links">Quick contents and links</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tutorials">Tutorials</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#detailed-instructions">Detailed instructions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="compilers.html">Available compilers</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="dgx.html">Nvidia DGX machines</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">Frequently asked questions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#libcuda-so-1-cannot-open-shared-object-file-no-such-file-or-directory">libcuda.so.1: cannot open shared object file: No such file or directory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#why-are-my-jobs-waiting-in-the-queue-with-reason-assocgrpmemrunminutes-assocgrpcpurunminutes-or-such">Why are my jobs waiting in the queue with reason AssocGrpMemRunMinutes/AssocGrpCPURunMinutes or such</a></li>
<li class="toctree-l4"><a class="reference internal" href="#why-are-my-jobs-in-state-launch-failed-requeued-held">Why are my jobs in state &#8220;launch failed requeued held&#8221;</a></li>
<li class="toctree-l4"><a class="reference internal" href="#invalid-account-error-message">Invalid account ... error message</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-find-out-the-remaining-runtime-of-my-job-allocation">How can I find out the remaining runtime of my job/allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#disk-quota-exceeded-error-but-i-have-plenty-of-space"><code class="docutils literal"><span class="pre">Disk</span> <span class="pre">quota</span> <span class="pre">exceeded</span></code> error but I have plenty of space</a></li>
<li class="toctree-l4"><a class="reference internal" href="#my-wrkdir-is-not-visible-on-my-department-computer">My $WRKDIR is not visible on my department computer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#while-copying-to-wrkdir-with-rsync-or-cp-i-m-getting-disk-quota-exceeded-error-though-my-quota-is-fine">While copying to $WRKDIR with rsync or cp I&#8217;m getting &#8216;Disk quota exceeded&#8217; error, though my quota is fine.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#can-i-change-zsh-to-bash">Can I change zsh to bash?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-fails-due-to-missed-module-environment-variables">Job fails due to missed module environment variables.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#there-seems-to-be-running-a-lot-of-jobs-in-the-short-queue-that-has-gone-for-longer-than-4-hours-should-that-be-possible">There seems to be running a lot of jobs in the short queue that has gone for longer than 4 hours. Should that be possible?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-print-my-text-file-to-a-local-department-printer">How can I print my text file to a local department printer?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-access-my-triton-files-from-outside">How can I access my Triton files from outside?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-copy-triton-files-from-outside-of-aalto">How can I copy Triton files from outside of Aalto?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-need-to-connect-to-some-server-on-a-node">I need to connect to some server on a node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Why are my jobs waiting in the queue with reason AssocGrpMemRunMinutes/AssocGrpCPURunMinutes or such</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Why are my jobs in state &#8220;launch failed requeued held&#8221;</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Invalid account ... error message</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">How can I find out the remaining runtime of my job/allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#disk-quota-exceeded-error"><code class="docutils literal"><span class="pre">Disk</span> <span class="pre">quota</span> <span class="pre">exceeded</span></code> error</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">While copying to $WRKDIR with rsync or cp I&#8217;m getting &#8216;Disk quota exceeded&#8217; error, though my quota is fine.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Can I change zsh to bash?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">Job fails due to missed module environment variables.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">There seems to be running a lot of jobs in the short queue that has gone for longer than 4 hours. Should that be possible?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">How can I print my text file to a local department printer?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">How can I access my Triton files from outside?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">How can I copy Triton files from outside of Aalto?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">I need to connect to some server on a node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#why-all-of-the-files-on-triton-cluster-are-in-one-color-how-can-i-make-them-colorful-like-green-for-execution-files-blue-for-folds">Why all of the files on triton cluster are in one color? How can I make them colorful? Like green for execution files, blue for folds</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-do-i-subscribe-to-triton-users-maillist">How do I subscribe to triton-users maillist?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-can-t-save-anything-to-my-home-directory-get-some-fsync-error">I can&#8217;t save anything to my <code class="docutils literal"><span class="pre">$HOME</span></code> directory, get some fsync error.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#what-node-names-like-cn-01-224-mean">What node names like cn[01-224] mean?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#what-is-a-good-scaling-factor-for-parallel-applications-what-is-the-recommended-number-of-processors-for-parallel-jobs">What is a good scaling factor for parallel applications? What is the recommended number of processors for parallel jobs?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#can-you-recovery-some-files-from-my-home-or-wrkdir-directory">Can you recovery some files from my <code class="docutils literal"><span class="pre">$HOME</span></code> or <code class="docutils literal"><span class="pre">$WRKDIR</span></code> directory?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-cluster-has-a-few-compiler-sets-which-one-i-suppose-to-use-what-are-the-limits-for-commercial-compilers">The cluster has a few compiler sets. Which one I suppose to use? What are the limits for commercial compilers?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code-is-compiled-with-shared-libraries-and-it-stops-with-an-error-message-error-while-loading-shared-libraries-libsome-so-cannot-open-shared-object-file-no-such-file-or-directory">Code is compiled with shared libraries and it stops with an error message: <code class="docutils literal"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libsome.so:</span> <span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#while-compiling-should-i-use-static-or-shared-version-of-some-library">While compiling should I use static or shared version of some library?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-ve-got-a-binary-file-may-i-find-out-somehow-whether-it-is-32-bit-or-64-bit-compiled">I&#8217;ve got a binary file, may I find out somehow whether it is 32-bit or 64-bit compiled?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#graphical-programs-don-t-work-x11-x">Graphical programs don&#8217;t work (X11, -X)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="general.html">Running programs on Triton</a></li>
<li class="toctree-l3"><a class="reference internal" href="gpu.html">GPU Computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="grid.html">Grid computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="grid2.html">Grid computing 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="jobs.html">Monitoring jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="libs.html">Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="localstorage.html">Storage: local drives</a></li>
<li class="toctree-l3"><a class="reference internal" href="lustre.html">Storage: Lustre (scratch)</a></li>
<li class="toctree-l3"><a class="reference internal" href="mpilibs.html">MPI on Triton</a></li>
<li class="toctree-l3"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="quotas.html">Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="singularity.html">Singularity Containers</a></li>
<li class="toctree-l3"><a class="reference internal" href="smallfiles.html">Small files</a></li>
<li class="toctree-l3"><a class="reference internal" href="storage.html">Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="toolchains.html">Compilers and toolchains</a></li>
<li class="toctree-l3"><a class="reference internal" href="workflows.html">Remote workflows at Aalto</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#reference-and-examples">Reference and Examples</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../scicomp/index.html">Scientific computing</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Training</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">About these docs</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Aalto scientific computing</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">Triton user guide</a> &raquo;</li>
      
    <li>Frequently asked questions</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="https://github.com/AaltoScienceIT/scicomp-docs/blob/master/triton/usage/faq.rst" class="fa fa-github"> Edit on GitHub</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="frequently-asked-questions">
<h1>Frequently asked questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h1>
<p>Frequently Asked Questions. The latest are at the beginning.</p>
<div class="section" id="libcuda-so-1-cannot-open-shared-object-file-no-such-file-or-directory">
<h2>libcuda.so.1: cannot open shared object file: No such file or directory<a class="headerlink" href="#libcuda-so-1-cannot-open-shared-object-file-no-such-file-or-directory" title="Permalink to this headline">¶</a></h2>
<p>You are trying to run a GPU program (using CUDA) on a node without a
GPU (and thus, no <code class="docutils literal"><span class="pre">libcuda.so.1</span></code>.  Remember to <a class="reference internal" href="../tut/gpu.html"><em>specify that
you need GPUs</em></a></p>
</div>
<div class="section" id="why-are-my-jobs-waiting-in-the-queue-with-reason-assocgrpmemrunminutes-assocgrpcpurunminutes-or-such">
<h2>Why are my jobs waiting in the queue with reason AssocGrpMemRunMinutes/AssocGrpCPURunMinutes or such<a class="headerlink" href="#why-are-my-jobs-waiting-in-the-queue-with-reason-assocgrpmemrunminutes-assocgrpcpurunminutes-or-such" title="Permalink to this headline">¶</a></h2>
<p>Accounts are limited in how much the can run at a time, in order to
prevent a single or a few users from hogging the entire cluster with
long-running jobs if it happens to be idle (e.g. after a service break).
The limit is such that it limits the maximum remaining runtime of all
the jobs of a user. So the way to run more jobs concurrently is to run
shorter and/or smaller (less CPU&#8217;s, less memory) jobs. For an in-depth
explanation see
<a class="reference external" href="http://tech.ryancox.net/2014/04/scheduler-limit-remaining-cputime-per.html">http://tech.ryancox.net/2014/04/scheduler-limit-remaining-cputime-per.html</a>
and for a graphical simulator you can play around with:
<a class="reference external" href="https://rc.byu.edu/simulation/grpcpurunmins.php">https://rc.byu.edu/simulation/grpcpurunmins.php</a> . You can see the
exact limits of your account with</p>
<div class="highlight-python"><div class="highlight"><pre>sacctmgr -s show user $USER format=user,account,grptresrunmins%70
</pre></div>
</div>
</div>
<div class="section" id="why-are-my-jobs-in-state-launch-failed-requeued-held">
<h2>Why are my jobs in state &#8220;launch failed requeued held&#8221;<a class="headerlink" href="#why-are-my-jobs-in-state-launch-failed-requeued-held" title="Permalink to this headline">¶</a></h2>
<p>Slurm is configured such that if a job fails due to some outside reason
(e.g. the node where it&#8217;s running fails rather than the job itself
crashing due to a bug in the job) the job is requeued in a held state.
If you&#8217;re sure that everything is ok again you can release the job for
scheduling with &#8220;scontrol release JOBID&#8221;. If you don&#8217;t want this
behavior (i.e. you&#8217;d prefer that such failed jobs would just disappear)
then you can prevent the requeuing with</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c1">#SBATCH --no-requeue</span>
</pre></div>
</div>
</div>
<div class="section" id="invalid-account-error-message">
<h2>Invalid account ... error message<a class="headerlink" href="#invalid-account-error-message" title="Permalink to this headline">¶</a></h2>
<p>While submitting a job you receive an error message like</p>
<div class="highlight-python"><div class="highlight"><pre>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified
</pre></div>
</div>
<p>Most probably your account is missing from SLURM database, to check it
out run</p>
<div class="highlight-python"><div class="highlight"><pre>$ sacctmgr show user $USER
      User   Def Acct     Admin
---------- ---------- ---------
  YOUR_LOGIN     YOUR_DEPART      None
</pre></div>
</div>
<p>That should return your login and associated department/school. If
empty, please contact your <a class="reference internal" href="../help.html"><em>local support team</em></a>
member and ask to add your account to SLURM db.</p>
</div>
<div class="section" id="how-can-i-find-out-the-remaining-runtime-of-my-job-allocation">
<h2>How can I find out the remaining runtime of my job/allocation<a class="headerlink" href="#how-can-i-find-out-the-remaining-runtime-of-my-job-allocation" title="Permalink to this headline">¶</a></h2>
<p>You can find out the remaining time of any job that is running with</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">squeue</span> <span class="o">-</span><span class="n">h</span> <span class="o">-</span><span class="n">j</span>  <span class="o">-</span><span class="n">o</span> <span class="o">%</span><span class="n">L</span>
</pre></div>
</div>
<p>Inside a job script or <em>sinteractive</em> session you can use the
environment variable SLURM_JOB_ID to refer to the current job ID.</p>
</div>
<div class="section" id="disk-quota-exceeded-error-but-i-have-plenty-of-space">
<h2><code class="docutils literal"><span class="pre">Disk</span> <span class="pre">quota</span> <span class="pre">exceeded</span></code> error but I have plenty of space<a class="headerlink" href="#disk-quota-exceeded-error-but-i-have-plenty-of-space" title="Permalink to this headline">¶</a></h2>
<p>Main article: <a class="reference internal" href="quotas.html"><em>Triton Quotas</em></a></p>
<p>Everyone should have a group quota, but no user quota. All files need to
be in a proper group (either a shared group with quota, or your &#8220;user
private group&#8221;). First of all, use the &#8216;quota&#8217; command to make sure that
neither disk space nor number of files are exceeded. Also, make sure
that you use $WRKDIR for data and not $HOME. If you actually need more
quota, ask us.</p>
<p><em>Solution:</em> add to your main directory and all your subdirectories to
the right group, and make sure all directories have the group s-bit set,
(SETGID bit, see <code class="docutils literal"><span class="pre">man</span> <span class="pre">chmod</span></code>). This means &#8220;any files created within
this directory get the directory&#8217;s group&#8221;. Since your default group is
&#8220;domain users&#8221; which has no quota, if the s-bit is not set, you get an
immediate quota exceeded by default.</p>
<div class="highlight-python"><div class="highlight"><pre># Fix everything
#  (only for $WRKDIR or group directories, still in testing):
/share/apps/bin/quotafix -sg --fix /path/to/dir/

# Manual fixing:
# Fix sticky bit:
lfs find $WRKDIR -type d --print0 | xargs -0 chmod g+s
# Fix group:
lfs find /path/to/dir  ! --group $GROUP -print0 | xargs -0 chgrp $GROUP
</pre></div>
</div>
<p><em>Why this happens:</em> $WRKDIR directory is owned by the user and user&#8217;s
group that has the same name and GID as UID. Quota is set per group, not
per user. That is how it was implemented since 2011 when we got Lustre
in use. Since spring 2015 Triton is using Aalto AD for the
authentication which sets everyone a default group ID to &#8216;domain users&#8217;.
If you copy anything to $WRKDIR/subdirectory that has no +s bit you copy
as a &#8216;domain users&#8217; member and file system refuses to do so due to no
quota available. If g+s bit is set, all your directories/files
copied/created will get the directory&#8217;s group ownership instead of that
default group &#8216;domain users&#8217;. There can be very confusing interactions
between this and user/shared directories.</p>
</div>
<div class="section" id="my-wrkdir-is-not-visible-on-my-department-computer">
<h2>My $WRKDIR is not visible on my department computer<a class="headerlink" href="#my-wrkdir-is-not-visible-on-my-department-computer" title="Permalink to this headline">¶</a></h2>
<p>Most likely your Kerberos ticket has expired. If you log in with a
password or use &#8216;kinit&#8217;, you can get an another ticket. See page on
<a class="reference internal" href="../tut/storage.html"><em>data storage</em></a> for more information.</p>
</div>
<div class="section" id="while-copying-to-wrkdir-with-rsync-or-cp-i-m-getting-disk-quota-exceeded-error-though-my-quota-is-fine">
<h2>While copying to $WRKDIR with rsync or cp I&#8217;m getting &#8216;Disk quota exceeded&#8217; error, though my quota is fine.<a class="headerlink" href="#while-copying-to-wrkdir-with-rsync-or-cp-i-m-getting-disk-quota-exceeded-error-though-my-quota-is-fine" title="Permalink to this headline">¶</a></h2>
<p>It is related to the above mentioned issue, something like rsync -a ...
or cp -p ... are trying to save original group ownership attribute,
which will not work. Try this instead:</p>
<div class="highlight-python"><div class="highlight"><pre>## mainly one should avoid -g (as well as -a) that preserves group attributes
$ rsync -urlptDxv --chmod=Dg+s somefile triton.aalto.fi:/path/to/work/directory

## avoid &#39;-p&#39; with cp, or if you want to keep timestapms, mode etc, then use &#39;--preserve=&#39;
$ cp -r --preserve=mode,timestamps  somefile /path/to/mounted/triton/work/directory
</pre></div>
</div>
</div>
<div class="section" id="can-i-change-zsh-to-bash">
<h2>Can I change zsh to bash?<a class="headerlink" href="#can-i-change-zsh-to-bash" title="Permalink to this headline">¶</a></h2>
<p>Yes. Change shell to your Aalto account and re-login to Triton to get
your newly changed shell to work. For Aalto account changes one can
login to kosh.aalto.fi, run <code class="docutils literal"><span class="pre">kinit</span></code> first and then run <code class="docutils literal"><span class="pre">chsh</span></code>, then
type /bin/bash. To find out what is your current shell, run
<code class="docutils literal"><span class="pre">echo</span> <span class="pre">$SHELL</span></code></p>
<p>For the record: your default shell is not set by Triton environment but
by your Aalto account.</p>
</div>
<div class="section" id="job-fails-due-to-missed-module-environment-variables">
<h2>Job fails due to missed module environment variables.<a class="headerlink" href="#job-fails-due-to-missed-module-environment-variables" title="Permalink to this headline">¶</a></h2>
<p>You have included &#8216;module load module/name&#8217; but job still fails due to
missing shared libraries or that it can not find some binary etc. That
is a known ZSH related issue. In your sbatch script please use <code class="docutils literal"><span class="pre">-l</span></code>
option (aka <code class="docutils literal"><span class="pre">--login</span></code>) which forces bash to read all the
initialization files at /etc/profile.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="ch">#!/bin/bash -l</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Alternatively, one can change shell from ZSH to BASH to avoid this
hacks, see the post above.</p>
</div>
<div class="section" id="there-seems-to-be-running-a-lot-of-jobs-in-the-short-queue-that-has-gone-for-longer-than-4-hours-should-that-be-possible">
<h2>There seems to be running a lot of jobs in the short queue that has gone for longer than 4 hours. Should that be possible?<a class="headerlink" href="#there-seems-to-be-running-a-lot-of-jobs-in-the-short-queue-that-has-gone-for-longer-than-4-hours-should-that-be-possible" title="Permalink to this headline">¶</a></h2>
<p>SLURM kills jobs based on the partition&#8217;s TimeLimit + OverTimeLimit
parameter. The later in our case is 60 minutes. If for instance queue
time limit is 4 hours, SLURM will allow to run on it 4 hours, plus 1
hour, thus no longer than 5 hours. Though OverTimeLimit may vary, don&#8217;t
rely on it. Partition&#8217;s (aka queue&#8217;s) TimeLimit is the one that end user
should take into account when submit his/her job. Time limits per
partiton one can check with <code class="docutils literal"><span class="pre">slurm</span> <span class="pre">p</span></code> command.</p>
<p>For setting up exact time frame after which you want your job to be
killed anyway, set <code class="docutils literal"><span class="pre">--time</span></code> parameter when submitting the job. When
the time limit is reached, each task in each job step is sent SIGTERM
followed by SIGKILL. If you run a parallel job, set <code class="docutils literal"><span class="pre">--time</span></code> with
<code class="docutils literal"><span class="pre">srun</span></code> as well. See &#8216;<code class="docutils literal"><span class="pre">man</span> <span class="pre">srun'</span></code> and &#8216;<code class="docutils literal"><span class="pre">man</span> <span class="pre">sbatch</span></code>&#8216; for details.</p>
<div class="highlight-python"><div class="highlight"><pre>#SBATCH --time=1:00:00
...

srun --time=1:00:00 ...
</pre></div>
</div>
</div>
<div class="section" id="how-can-i-print-my-text-file-to-a-local-department-printer">
<h2>How can I print my text file to a local department printer?<a class="headerlink" href="#how-can-i-print-my-text-file-to-a-local-department-printer" title="Permalink to this headline">¶</a></h2>
<p>We don&#8217;t have local department printers configured anywhere on Triton.
But one can use SSH magic to send a file or command output to a remote
printer. Run from your local workstation, insert the target printer
name:</p>
<div class="highlight-python"><div class="highlight"><pre>... printing text file
$ ssh user@triton.aalto.fi &quot;cat file.txt&quot; | enscript -P printer_name
... printing a PostScript file
$ ssh user@triton.aalto.fi &quot;cat file.ps&quot; | lp -d printer_name -
... printing a man page
$ ssh user@triton.aalto.fi &quot;man -t sbatch&quot; | lp -d printer_name -
</pre></div>
</div>
</div>
<div class="section" id="how-can-i-access-my-triton-files-from-outside">
<h2>How can I access my Triton files from outside?<a class="headerlink" href="#how-can-i-access-my-triton-files-from-outside" title="Permalink to this headline">¶</a></h2>
<div class="section" id="remote-mounting">
<h3>Remote mounting<a class="headerlink" href="#remote-mounting" title="Permalink to this headline">¶</a></h3>
<p>The scratch filesystem can be mounted from inside the Aalto networks
by using <code class="docutils literal"><span class="pre">smb://data.triton.aalto.fi/scratch/</span></code>.  For example, from
Nautilus (the file manager) on Ubuntu, use &#8220;File&#8221; -&gt; &#8220;Connect to
server&#8221;.  Outside Aalto networks, use the Aalto VPN.  If it is not an
Aalto computer, you may need to us <code class="docutils literal"><span class="pre">AALTO\username</span></code> as the username,
and your Aalto password.</p>
<p>Or you can use <code class="docutils literal"><span class="pre">sshfs</span></code> – filesystem client based on SSH. Most Linux workstations
have it installed by default, if not, install it or ask your local IT
support to do it for you. For setting up your SSHFS mount from your
local workstation: create a local directory and mount remote directory
with sshfs</p>
<div class="highlight-python"><div class="highlight"><pre>$ mkdir /LOCALDIR/triton
$ sshfs user1@triton.aalto.fi:/triton/PATH/TO/DIR /LOCALDIR/triton
</pre></div>
</div>
<p>Replace <code class="docutils literal"><span class="pre">user1</span></code> with your real username and <code class="docutils literal"><span class="pre">/LOCALDIR</span></code> with
a real directory on your local drive. After successful mount, use you
/LOCALDIR <code class="docutils literal"><span class="pre">/triton</span></code>  directory as it would be local. To unmount it,
run <code class="docutils literal"><span class="pre">fusermount</span> <span class="pre">-u</span> <span class="pre">/LOCALDIR/triton</span></code>.</p>
<p>PHYS users example, assuming that Triton and PHYS accounts are the same:</p>
<div class="highlight-python"><div class="highlight"><pre>$ mkdir /localwrk/$USER/triton
$ sshfs triton.aalto.fi:/triton/tfy/work/$USER  /localwrk/$USER/triton
$ cd /localwrk/$USER/triton
... (do what you need, and then unmount when there is no need any more)
$ fusermount -u /localwrk/$USER/triton
</pre></div>
</div>
<p><strong>Easy access with Nautilus</strong></p>
<p>The SSHFS method described above works from any console. Though in case
of Linux desktops, when one has a GUI like Gnome or Unity (read all
Ubuntu users) one may use Nautilus – default file manager &#8211; to mount
remote SSH directory. Click <code class="docutils literal"><span class="pre">File</span> <span class="pre">-&gt;</span> <span class="pre">Connect</span> <span class="pre">to</span> <span class="pre">Server</span></code> choose
<code class="docutils literal"><span class="pre">SSH</span></code>, input triton.aalto.fi as a server and directory
<code class="docutils literal"><span class="pre">/triton/PATH/TO/DIR</span></code> you&#8217;d like to mount, type your name. Leave
password field empty if you use SSH key. As soon as Nautilus will
establish connection it will appear on the left-hand side below Network
header. Now you may access it as it would be your local directory. To
keep it as a bookmark click on the mount point and press <code class="docutils literal"><span class="pre">Ctrl+D</span></code>, it
will appear below Bookmark header on the same menu.</p>
</div>
<div class="section" id="copying-files">
<h3>Copying files<a class="headerlink" href="#copying-files" title="Permalink to this headline">¶</a></h3>
<p>If your workstatios has no NFS mounts from Triton (CS and NBE have,
consult with your local admins for exact paths), you may always use
SSH.  Either copy your files from triton to a local directory on your
workstation, like:</p>
<div class="highlight-python"><div class="highlight"><pre>$ sftp user1@triton.aalto.fi:/triton/path/to/dir/* .
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-can-i-copy-triton-files-from-outside-of-aalto">
<h2>How can I copy Triton files from outside of Aalto?<a class="headerlink" href="#how-can-i-copy-triton-files-from-outside-of-aalto" title="Permalink to this headline">¶</a></h2>
<p>It is an extension of the previous question. In case you are outside
of Aalto and has neither direct access to Triton nor access to NFS
mounted directories on your directory servers. Say you want to copy
your Triton files to your home workstation. It could be done by
setting up an SSH tunnel to your department SSH server. A few steps to
be done: set tunnel to your local department server, then from your
department server to Triton, and then run any rsync/sftp/ssh command
you want from your client using that tunnel. The tunnel should be up
during whole session.</p>
<div class="highlight-python"><div class="highlight"><pre>client: ssh -L9509:localhost:9509 department.ssh.server
department server: ssh -L9509:localhost:22 triton.aalto.fi
client: sftp -P 9509 localhost:/triton/own/dir/* /local/dir
</pre></div>
</div>
<p>Note that port 9509 is taken for example only. One can use any other
available port. Alaternatively, if you have a Linux or Mac OS X machine,
you can setup a &#8220;proxy command&#8221;, so you don&#8217;t have to do the steps above
manually everytime. On your home machine/laptop, in the file
~/.ssh/config put the lines</p>
<div class="highlight-python"><div class="highlight"><pre>Host triton
    ProxyCommand /usr/bin/ssh DEPARTMENTUSERNAME@department.ssh.server &quot;/usr/bin/nc -w 10 triton.aalto.fi 22&quot;
    User TRITONUSERNAME
</pre></div>
</div>
<p>This creates a host alias &#8220;triton&#8221; that is proxied via the department
server. So you can copy a file from your home machine/laptop to triton
with a command like:</p>
<div class="highlight-python"><div class="highlight"><pre>rsync filename triton:remote_filename
</pre></div>
</div>
</div>
<div class="section" id="i-need-to-connect-to-some-server-on-a-node">
<span id="faq-connecttoserveronnode"></span><h2>I need to connect to some server on a node<a class="headerlink" href="#i-need-to-connect-to-some-server-on-a-node" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s say you have some server (e.g. debugging server, notebook server,
...) running on a node.</p>
</div>
<div class="section" id="id1">
<h2>Why are my jobs waiting in the queue with reason AssocGrpMemRunMinutes/AssocGrpCPURunMinutes or such<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Accounts are limited in how much the can run at a time, in order to
prevent a single or a few users from hogging the entire cluster with
long-running jobs if it happens to be idle (e.g. after a service break).
The limit is such that it limits the maximum remaining runtime of all
the jobs of a user. So the way to run more jobs concurrently is to run
shorter and/or smaller (less CPU&#8217;s, less memory) jobs. For an in-depth
explanation see
<a class="reference external" href="http://tech.ryancox.net/2014/04/scheduler-limit-remaining-cputime-per.html">http://tech.ryancox.net/2014/04/scheduler-limit-remaining-cputime-per.html</a>
and for a graphical simulator you can play around with:
<a class="reference external" href="https://rc.byu.edu/simulation/grpcpurunmins.php">https://rc.byu.edu/simulation/grpcpurunmins.php</a> . You can see the
exact limits of your account with</p>
<div class="highlight-python"><div class="highlight"><pre>sacctmgr -s show user $USER format=user,account,grptresrunmins%70
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h2>Why are my jobs in state &#8220;launch failed requeued held&#8221;<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Slurm is configured such that if a job fails due to some outside reason
(e.g. the node where it&#8217;s running fails rather than the job itself
crashing due to a bug in the job) the job is requeued in a held state.
If you&#8217;re sure that everything is ok again you can release the job for
scheduling with &#8220;scontrol release JOBID&#8221;. If you don&#8217;t want this
behavior (i.e. you&#8217;d prefer that such failed jobs would just disappear)
then you can prevent the requeuing with</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c1">#SBATCH --no-requeue</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2>Invalid account ... error message<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>While submitting a job you receive an error message like</p>
<div class="highlight-python"><div class="highlight"><pre>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified
</pre></div>
</div>
<p>Most probably your account is missing from SLURM database, to check it
out run</p>
<div class="highlight-python"><div class="highlight"><pre>$ sacctmgr show user $USER
      User   Def Acct     Admin
---------- ---------- ---------
  YOUR_LOGIN     YOUR_DEPART      None
</pre></div>
</div>
<p>That should return your login and associated department/school. If
empty, please contact your <a class="reference internal" href="../help.html"><em>local support team</em></a>
member and ask to add your account to SLURM db.</p>
</div>
<div class="section" id="id4">
<h2>How can I find out the remaining runtime of my job/allocation<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>You can find out the remaining time of any job that is running with</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">squeue</span> <span class="o">-</span><span class="n">h</span> <span class="o">-</span><span class="n">j</span>  <span class="o">-</span><span class="n">o</span> <span class="o">%</span><span class="n">L</span>
</pre></div>
</div>
<p>Inside a job script or <em>sinteractive</em> session you can use the
environment variable SLURM_JOB_ID to refer to the current job ID.</p>
</div>
<div class="section" id="disk-quota-exceeded-error">
<h2><code class="docutils literal"><span class="pre">Disk</span> <span class="pre">quota</span> <span class="pre">exceeded</span></code> error<a class="headerlink" href="#disk-quota-exceeded-error" title="Permalink to this headline">¶</a></h2>
<p><em>Main article: `Triton Quotas &lt;quotas&gt;`* *and</em></p>
<div class="highlight-python"><div class="highlight"><pre>space exceeded but I have plenty of space
</pre></div>
</div>
<p>Everyone should have a group quota, but no user quota. All files need to
be in a proper group (either a shared group with quota, or your &#8220;user
private group&#8221;). First of all, use the &#8216;quota&#8217; command to make sure that
neither disk space nor number of files are exceeded. Also, make sure
that you use $WRKDIR for data and not $HOME. If you actually need more
quota, ask us.</p>
<p><em>Solution:</em> add to your main directory and all your subdirectories to
the right group, and make sure all directories have the group s-bit set,
(SETGID bit, see <code class="docutils literal"><span class="pre">man</span> <span class="pre">chmod</span></code>). This means &#8220;any files created within
this directory get the directory&#8217;s group&#8221;. Since your default group is
&#8220;domain users&#8221; which has no quota, if the s-bit is not set, you get an
immediate quota exceeded by default.</p>
<div class="highlight-python"><div class="highlight"><pre># Fix everything
#  (only for $WRKDIR or group directories, still in testing):
/share/apps/bin/quotafix -sg --fix /path/to/dir/

# Manual fixing:
# Fix sticky bit:
lfs find $WRKDIR -type d --print0 | xargs -0 chmod g+s
# Fix group:
lfs find /path/to/dir  ! --group $GROUP -print0 | xargs -0 chgrp $GROUP
</pre></div>
</div>
<p><em>Why this happens:</em> $WRKDIR directory is owned by the user and user&#8217;s
group that has the same name and GID as UID. Quota is set per group, not
per user. That is how it was implemented since 2011 when we got Lustre
in use. Since spring 2015 Triton is using Aalto AD for the
authentication which sets everyone a default group ID to &#8216;domain users&#8217;.
If you copy anything to $WRKDIR/subdirectory that has no +s bit you copy
as a &#8216;domain users&#8217; member and file system refuses to do so due to no
quota available. If g+s bit is set, all your directories/files
copied/created will get the directory&#8217;s group ownership instead of that
default group &#8216;domain users&#8217;. There can be very confusing interactions
between this and user/shared directories.</p>
</div>
<div class="section" id="id5">
<h2>While copying to $WRKDIR with rsync or cp I&#8217;m getting &#8216;Disk quota exceeded&#8217; error, though my quota is fine.<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>It is related to the above mentioned issue, something like rsync -a ...
or cp -p ... are trying to save original group ownership attribute,
which will not work. Try this instead:</p>
<div class="highlight-python"><div class="highlight"><pre>## mainly one should avoid -g (as well as -a) that preserves group attributes
$ rsync -urlptDxv --chmod=Dg+s somefile triton.aalto.fi:/path/to/work/directory

## avoid &#39;-p&#39; with cp, or if you want to keep timestapms, mode etc, then use &#39;--preserve=&#39;
$ cp -r --preserve=mode,timestamps  somefile /path/to/mounted/triton/work/directory
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h2>Can I change zsh to bash?<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>Yes. Change shell to your Aalto account and re-login to Triton to get
your newly changed shell to work. For Aalto account changes one can
login to <code class="docutils literal"><span class="pre">kosh.aalto.fi</span></code>, run <code class="docutils literal"><span class="pre">kinit</span></code> first
and then run <code class="docutils literal"><span class="pre">chsh</span></code>, then type /bin/bash. To find out what is your
current shell, run <code class="docutils literal"><span class="pre">echo</span> <span class="pre">$SHELL</span></code></p>
<p>For the record: your default shell is not set by Triton environment but
by your Aalto account.</p>
</div>
<div class="section" id="id7">
<h2>Job fails due to missed module environment variables.<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>You have included &#8216;module load module/name&#8217; but job still fails due to
missing shared libraries or that it can not find some binary etc. That
is a known ZSH related issue. In your sbatch script please use <code class="docutils literal"><span class="pre">-l</span></code>
option (aka <code class="docutils literal"><span class="pre">--login</span></code>) which forces bash to read all the
initialization files at /etc/profile.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="ch">#!/bin/bash -l</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Alternatively, one can change shell from ZSH to BASH to avoid this
hacks, see the post above.</p>
</div>
<div class="section" id="id8">
<h2>There seems to be running a lot of jobs in the short queue that has gone for longer than 4 hours. Should that be possible?<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>SLURM kills jobs based on the partition&#8217;s TimeLimit + OverTimeLimit
parameter. The later in our case is 60 minutes. If for instance queue
time limit is 4 hours, SLURM will allow to run on it 4 hours, plus 1
hour, thus no longer than 5 hours. Though OverTimeLimit may vary, don&#8217;t
rely on it. Partition&#8217;s (aka queue&#8217;s) TimeLimit is the one that end user
should take into account when submit his/her job. Time limits per
partiton one can check with <code class="docutils literal"><span class="pre">slurm</span> <span class="pre">p</span></code> command.</p>
<p>For setting up exact time frame after which you want your job to be
killed anyway, set <code class="docutils literal"><span class="pre">--time</span></code> parameter when submitting the job. When
the time limit is reached, each task in each job step is sent SIGTERM
followed by SIGKILL. If you run a parallel job, set <code class="docutils literal"><span class="pre">--time</span></code> with
<code class="docutils literal"><span class="pre">srun</span></code> as well. See &#8216;<code class="docutils literal"><span class="pre">man</span> <span class="pre">srun'</span></code> and &#8216;<code class="docutils literal"><span class="pre">man</span> <span class="pre">sbatch</span></code>&#8216; for details.</p>
<div class="highlight-python"><div class="highlight"><pre>#SBATCH --time=1:00:00
...

srun --time=1:00:00 ...
</pre></div>
</div>
</div>
<div class="section" id="id9">
<h2>How can I print my text file to a local department printer?<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>We don&#8217;t have local department printers configured anywhere on Triton.
But one can use SSH magic to send a file or command output to a remote
printer. Run from your local workstation, insert the target printer
name:</p>
<div class="highlight-python"><div class="highlight"><pre>... printing text file
$ ssh user@triton.aalto.fi &quot;cat file.txt&quot; | enscript -P printer_name
... printing a PostScript file
$ ssh user@triton.aalto.fi &quot;cat file.ps&quot; | lp -d printer_name -
... printing a man page
$ ssh user@triton.aalto.fi &quot;man -t sbatch&quot; | lp -d printer_name -
</pre></div>
</div>
</div>
<div class="section" id="id10">
<h2>How can I access my Triton files from outside?<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>If your workstatios has no NFS mounts from Triton (CS and NBE have,
consult with your local admins for exact paths), you may always use
SSH.  Either copy your files from triton to a local directory on your
workstation, like</p>
<div class="highlight-python"><div class="highlight"><pre>$ rsync -pr user1@triton.aalto.fi:/triton/path/to/dir .
</pre></div>
</div>
<p>or use SSHFS – filesystem client based on SSH. Most Linux workstations
have it installed by default, if not, install it or ask your local IT
support to do it for you. For setting up your SSHFS mount from your
local workstation: create a local directory and mount remote directory
with sshfs</p>
<div class="highlight-python"><div class="highlight"><pre>$ mkdir /LOCALDIR/triton
$ sshfs user1@triton.aalto.fi:/triton/PATH/TO/DIR /LOCALDIR/triton
</pre></div>
</div>
<p>Replace <code class="docutils literal"><span class="pre">user1</span></code> with your real username and <code class="docutils literal"><span class="pre">/LOCALDIR</span></code> with
a real directory on your local drive. After successful mount, use you
/LOCALDIR<code class="docutils literal"><span class="pre">/triton</span></code> directory as it would be local. To unmount it,
run <code class="docutils literal"><span class="pre">fusermount</span> <span class="pre">-u</span> <span class="pre">/LOCALDIR/triton</span></code> .</p>
<p>PHYS users example, assuming that Triton and PHYS accounts are the same:</p>
<div class="highlight-python"><div class="highlight"><pre>$ mkdir /localwrk/$USER/triton
$ sshfs triton.aalto.fi:/triton/tfy/work/$USER  /localwrk/$USER/triton
$ cd /localwrk/$USER/triton
... (do what you need, and then unmount when there is no need any more)
$ fusermount -u /localwrk/$USER/triton
</pre></div>
</div>
<p><strong>Easy access with Nautilus</strong></p>
<p>The SSHFS method described above works from any console. Though in case
of Linux desktops, when one has a GUI like Gnome or Unity (read all
Ubuntu users) one may use Nautilus – default file manager &#8211; to mount
remote SSH directory. Click <code class="docutils literal"><span class="pre">File</span> <span class="pre">-&gt;</span> <span class="pre">Connect</span> <span class="pre">to</span> <span class="pre">Server</span></code> choose
<code class="docutils literal"><span class="pre">SSH</span></code>, input <code class="docutils literal"><span class="pre">triton.aalto.fi</span></code> as a
server and directory <code class="docutils literal"><span class="pre">/triton/PATH/TO/DIR</span></code> you&#8217;d like to mount,
type your name. Leave password field empty if you use SSH key. As soon
as Nautilus will establish connection it will appear on the left-hand
side below Network header. Now you may access it as it would be your
local directory. To keep it as a bookmark click on the mount point and
press <code class="docutils literal"><span class="pre">Ctrl+D</span></code>, it will appear below Bookmark header on the same menu.</p>
</div>
<div class="section" id="id11">
<h2>How can I copy Triton files from outside of Aalto?<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>It is an extension of the previous question. In case you are outside
of Aalto and has neither direct access to Triton nor access to NFS
mounted directories on your directory servers. Say you want to copy
your Triton files to your home workstation. It could be done by
setting up an SSH tunnel to your department SSH server. A few steps to
be done: set tunnel to your local department server, then from your
department server to Triton, and then run any rsync/sftp/ssh command
you want from your client using that tunnel. The tunnel should be up
during whole session.</p>
<div class="highlight-python"><div class="highlight"><pre>client: ssh -L9509:localhost:9509 department.ssh.server
department server: ssh -L9509:localhost:22 triton.aalto.fi
client: sftp -P 9509 localhost:/triton/own/dir/* /local/dir
</pre></div>
</div>
<p>Note that port 9509 is taken for example only. One can use any other
available port. Alaternatively, if you have a Linux or Mac OS X machine,
you can setup a &#8220;proxy command&#8221;, so you don&#8217;t have to do the steps above
manually everytime. On your home machine/laptop, in the file
~/.ssh/config put the lines</p>
<div class="highlight-python"><div class="highlight"><pre>Host triton
    ProxyCommand /usr/bin/ssh DEPARTMENTUSERNAME@department.ssh.server &quot;/usr/bin/nc -w 10 triton.aalto.fi 22&quot;
    User TRITONUSERNAME
</pre></div>
</div>
<p>This creates a host alias &#8220;triton&#8221; that is proxied via the department
server. So you can copy a file from your home machine/laptop to triton
with a command like:</p>
<div class="highlight-python"><div class="highlight"><pre>rsync filename triton:remote_filename
</pre></div>
</div>
</div>
<div class="section" id="id12">
<h2>I need to connect to some server on a node<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s say you have some server (e.g. debugging server, notebook server,
...) running on a node. As usual, you can do this with ssh using port
forwarding. It is the same principle as in several of the above
questions.</p>
<p>For example, you want to connect from your own computer to port <code class="docutils literal"><span class="pre">AAAA</span></code>
on node <code class="docutils literal"><span class="pre">nnnNNN</span></code>. You run this command:</p>
<div class="highlight-python"><div class="highlight"><pre>ssh -L BBBB:nnnNNN:AAAA username@triton.aalto.fi
</pre></div>
</div>
<p>Then, when you connect to port <code class="docutils literal"><span class="pre">BBBB</span></code> on your own computer
(<code class="docutils literal"><span class="pre">localhost</span></code>, it gets forwarded straight to port <code class="docutils literal"><span class="pre">AAAA</span></code> on node
<code class="docutils literal"><span class="pre">nnnNNN</span></code>. Thus only one ssh connection gets us to any node. It is
possible for <code class="docutils literal"><span class="pre">BBBB</span></code> to be the same as <code class="docutils literal"><span class="pre">AAAA</span></code>. By the way, this works
with any type of connection. The node has to be listening on any
interface, not just the local interface. To connect to
<code class="docutils literal"><span class="pre">localhost:AAAA</span></code> on a node, you need to repeat the above steps twice
to forward from workstation-&gt;login and login-&gt;node, with the second
<code class="docutils literal"><span class="pre">nnnNNN</span></code> being <code class="docutils literal"><span class="pre">localhost</span></code>.</p>
</div>
<div class="section" id="why-all-of-the-files-on-triton-cluster-are-in-one-color-how-can-i-make-them-colorful-like-green-for-execution-files-blue-for-folds">
<h2>Why all of the files on triton cluster are in one color? How can I make them colorful? Like green for execution files, blue for folds<a class="headerlink" href="#why-all-of-the-files-on-triton-cluster-are-in-one-color-how-can-i-make-them-colorful-like-green-for-execution-files-blue-for-folds" title="Permalink to this headline">¶</a></h2>
<p>That is made intentionally due to high load on Lustre filesystem. Being
a high performance filesystem Lustre still has its own bottlenecks, and
one of the common Lustre troublemakers are <code class="docutils literal"><span class="pre">ls</span> <span class="pre">-lr</span></code> or <code class="docutils literal"><span class="pre">ls</span> <span class="pre">--color</span></code>
which generate lots of requests to Lustre meta servers which regular
usage by all users may get whole system in stuck. Please follow the
recommendations given at the last section at <a class="reference internal" href="lustre.html"><em>Data storage on the Lustre
file system</em></a></p>
</div>
<div class="section" id="how-do-i-subscribe-to-triton-users-maillist">
<h2>How do I subscribe to triton-users maillist?<a class="headerlink" href="#how-do-i-subscribe-to-triton-users-maillist" title="Permalink to this headline">¶</a></h2>
<p>Having a user account on Triton also means being on the
triton-users at aalto.fi mailist. That is where support team sends
all the Triton related announcements. All the Triton users MUST be
subscibed to the list. It is automatically kept up to date these days,
but just in case you are not yet there, please send
an email to your local team member and ask to add your email.</p>
<p>How to unsubscribe? You will be removed from the maillist as soon as
your Triton account is deleted from the system. Otherwise no way,
since we can&#8217;t notify about urgent things that affect data integrity
or other issues.</p>
</div>
<div class="section" id="i-can-t-save-anything-to-my-home-directory-get-some-fsync-error">
<h2>I can&#8217;t save anything to my <code class="docutils literal"><span class="pre">$HOME</span></code> directory, get some fsync error.<a class="headerlink" href="#i-can-t-save-anything-to-my-home-directory-get-some-fsync-error" title="Permalink to this headline">¶</a></h2>
<p>Most probably your quota has exceeded, check it out with <code class="docutils literal"><span class="pre">quota</span></code>
command.</p>
<p><code class="docutils literal"><span class="pre">quota</span></code> is a wrapper at <code class="docutils literal"><span class="pre">/usr/local/bin/quota</span></code> on front end which
merges output from classic quota utility that supports NFS and Lustre&#8217;s
<code class="docutils literal"><span class="pre">lfs</span> <span class="pre">quota</span></code>. NFS <code class="docutils literal"><span class="pre">$HOME</span></code> directory is limited to 10GB for everyone
and intended for initialization files mainly. Grace period is set to 7
days and &#8220;hard&#8221; quota is set to 11GB, which means you may exceed your
10GB quota by 1GB and have 7 days to go below 10GB again. However none
can exceed 11GB limit.</p>
<p>Note: Lustre mounted under <code class="docutils literal"><span class="pre">/triton</span></code> is the right place for your
simulation files. It is fast and has large quotas.</p>
</div>
<div class="section" id="what-node-names-like-cn-01-224-mean">
<h2>What node names like cn[01-224] mean?<a class="headerlink" href="#what-node-names-like-cn-01-224-mean" title="Permalink to this headline">¶</a></h2>
<p>All the hardware delivered by the vendor has been labeled with some
short name. In particular every single compute node has a label like
Cn01 or GPU001 etc. we used this notation to name compute nodes, that is
cn01 is just a hostname for Cn01, gpu001 is a hostname for GPU001 etc.
Shorthands like cn[01-224] mean all the hostnames in the range cn01,
cn02, cn03 .. cn224. Same for gpu[001-008], tb[003-008], fn[01-02].
Similar notations can be used with SLURM commands like:</p>
<div class="highlight-python"><div class="highlight"><pre>$ scontrol show node cn[01-12]
</pre></div>
</div>
</div>
<div class="section" id="what-is-a-good-scaling-factor-for-parallel-applications-what-is-the-recommended-number-of-processors-for-parallel-jobs">
<h2>What is a good scaling factor for parallel applications? What is the recommended number of processors for parallel jobs?<a class="headerlink" href="#what-is-a-good-scaling-factor-for-parallel-applications-what-is-the-recommended-number-of-processors-for-parallel-jobs" title="Permalink to this headline">¶</a></h2>
<div class="line-block">
<div class="line">The good scaling factor is 1.5 or higher. It means that your program
is running 1.5 times faster when you double the number of nodes.</div>
<div class="line">There is no way to know in advance the exact &#8220;universal&#8221; optimal
number of CPUs. It dependes on many factors, like the application
itself, type of MPI libraries, the initial input, I/O volume and the
current network state. Certainly, you must not expect that, as many
CPUs your application has got, that faster it will run. In general the
scaling on Triton is good since we have Infiniband for nodes
interconnect and DDN / Lustre for I/O.</div>
</div>
<p>Few recommendations about CPU number:</p>
<ul class="simple">
<li>benchmark your applications on different number of CPU cores 1, 2,
12, 24, 36, and larger. Check out with the developers, your
application may have ready scalability benchmarks and recommendations
for compiler, MPI libraries choice.</li>
<li>benchmark on shared memory i.e. up to 12 CPU cores within one node
and then on different nodes (distributed memory): involving
interconnect make have huge difference</li>
<li>if you are not sure about program scalability and you have no time
for testing, don&#8217;t run on more than 12 CPU cores within one node</li>
<li>be considerate! it is not you against others! do not try to fill up
the cluster just for being cool</li>
</ul>
</div>
<div class="section" id="can-you-recovery-some-files-from-my-home-or-wrkdir-directory">
<h2>Can you recovery some files from my <code class="docutils literal"><span class="pre">$HOME</span></code> or <code class="docutils literal"><span class="pre">$WRKDIR</span></code> directory?<a class="headerlink" href="#can-you-recovery-some-files-from-my-home-or-wrkdir-directory" title="Permalink to this headline">¶</a></h2>
<p>Short answer: yes for $HOME directory and no for $WRKDIR.</p>
<div class="line-block">
<div class="line">$HOME is slow NFS with small quota mounted through Ethernet. Intended
mainly for user initialization files and for some plain configs. We
make regular backups from <code class="docutils literal"><span class="pre">$HOME</span></code>.</div>
<div class="line"><code class="docutils literal"><span class="pre">$WRKDIR</span></code> (aka <code class="docutils literal"><span class="pre">/triton</span></code>) is fast Lustre, has large quota, mounted
through InfiniBand. Though no backups made from <code class="docutils literal"><span class="pre">/triton</span></code>, the DDN
storage system as such is secure and safe place for your data, though
you can always loose your data deleting them by mistake. Every user
must take care about his work files himself. We provide as much
diskspace to every user, as one needs and the amount of data is
growing rapidly. That is the reason why the user should manage his
important data himself. Consider backups of your valuable data on
DVDs/ USB drives or other resources outside of Triton.</div>
</div>
</div>
<div class="section" id="the-cluster-has-a-few-compiler-sets-which-one-i-suppose-to-use-what-are-the-limits-for-commercial-compilers">
<h2>The cluster has a few compiler sets. Which one I suppose to use? What are the limits for commercial compilers?<a class="headerlink" href="#the-cluster-has-a-few-compiler-sets-which-one-i-suppose-to-use-what-are-the-limits-for-commercial-compilers" title="Permalink to this headline">¶</a></h2>
<p>Currently there are two different sets of compilers: (i) GNU compilers,
native for Linux, installed by default, (ii) Intel compilers plus MKL, a
commercial suite, often the fastest compiler on Xeons.</p>
<p>FGI provides all FGI sites with 7 Intel licenses, thus only 7 users can
compile/link with Intel at once.</p>
</div>
<div class="section" id="code-is-compiled-with-shared-libraries-and-it-stops-with-an-error-message-error-while-loading-shared-libraries-libsome-so-cannot-open-shared-object-file-no-such-file-or-directory">
<h2>Code is compiled with shared libraries and it stops with an error message: <code class="docutils literal"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libsome.so:</span> <span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code><a class="headerlink" href="#code-is-compiled-with-shared-libraries-and-it-stops-with-an-error-message-error-while-loading-shared-libraries-libsome-so-cannot-open-shared-object-file-no-such-file-or-directory" title="Permalink to this headline">¶</a></h2>
<p>That means your program can&#8217;t find libraries which has been used at
linking/compiling time. You may always check shared library
dependencies:</p>
<div class="highlight-python"><div class="highlight"><pre>$ ldd YOUR_PROGRAM # print the list of libraries required by program
</pre></div>
</div>
<div class="line-block">
<div class="line">If some of libraries is marked as not found, then you should first (i)
find the exact path to that lib (suppose it is installed), then second
(ii) explicitly add it to your environment variable
$LD_LIBRARY_PATH.</div>
<div class="line">For instance, if your code has been previously compiled with the
<code class="docutils literal"><span class="pre">libmpi.so.0</span></code> but on SL6.2 it reports an error like
<code class="docutils literal"><span class="pre">error</span> <span class="pre">while</span> <span class="pre">loading</span> <span class="pre">shared</span> <span class="pre">libraries:</span> <span class="pre">libmpi.so.0</span></code> try to locate
the library:</div>
</div>
<div class="highlight-python"><div class="highlight"><pre>$ locate libmpi.so.0
/usr/lib64/compat-openmpi/lib/libmpi.so.0
/usr/lib64/compat-openmpi/lib/libmpi.so.0.0.2
</pre></div>
</div>
<p>and the add it to your <code class="docutils literal"><span class="pre">$LD_LIBRARY_PATH</span></code></p>
<div class="highlight-python"><div class="highlight"><pre>export LD_LIBRARY_PATH=/usr/lib64/compat-openmpi/lib:$LD_LIBARY_PATH # export the lib in BASH environment
</pre></div>
</div>
<p>or, as in case of <code class="docutils literal"><span class="pre">libmpi.so.0</span></code> we have ready
module config, just run</p>
<div class="highlight-python"><div class="highlight"><pre>module load compat-openmpi-x86_64
</pre></div>
</div>
<p>In case your code is missing some specific libs, not installed on Triton
(say you got a binary compiled from somewhere else), you have a few
choices: (i) get statically linked program or (ii) find/download missing
libs (for instance from developers&#8217; site). For the second, copy libs to
your $WRKDIR and add paths to <code class="docutils literal"><span class="pre">$LD_LIBRARY_PATH</span></code>, in the same maner as
described above.</p>
<p>See also:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ldconfig</span> <span class="o">-</span><span class="n">p</span> <span class="c1"># print the list of system-wide available shared libraries</span>
</pre></div>
</div>
</div>
<div class="section" id="while-compiling-should-i-use-static-or-shared-version-of-some-library">
<h2>While compiling should I use static or shared version of some library?<a class="headerlink" href="#while-compiling-should-i-use-static-or-shared-version-of-some-library" title="Permalink to this headline">¶</a></h2>
<p>One can use both, though for shared libs all your linked libs must be
either in your <code class="docutils literal"><span class="pre">$WRKDIR</span></code> in <code class="docutils literal"><span class="pre">/shared/apps</span></code> or must be installed by
default on all the compute nodes like vast majority of GCC and other
default Linux libs.</p>
</div>
<div class="section" id="i-ve-got-a-binary-file-may-i-find-out-somehow-whether-it-is-32-bit-or-64-bit-compiled">
<h2>I&#8217;ve got a binary file, may I find out somehow whether it is 32-bit or 64-bit compiled?<a class="headerlink" href="#i-ve-got-a-binary-file-may-i-find-out-somehow-whether-it-is-32-bit-or-64-bit-compiled" title="Permalink to this headline">¶</a></h2>
<p>Use <code class="docutils literal"><span class="pre">file</span></code> utility:</p>
<div class="highlight-python"><div class="highlight"><pre># file /usr/bin/gcc
/usr/bin/gcc: ELF 64-bit LSB executable, AMD x86-64, version 1 (SYSV),
for GNU/Linux 2.4.0, dynamically linked (uses shared libs), not stripped
</pre></div>
</div>
<p>it displays the type of an executable or object file.</p>
</div>
<div class="section" id="graphical-programs-don-t-work-x11-x">
<h2>Graphical programs don&#8217;t work (X11, -X)<a class="headerlink" href="#graphical-programs-don-t-work-x11-x" title="Permalink to this headline">¶</a></h2>
<p>In order for graphical programs on Linux to work, a file
<code class="docutils literal"><span class="pre">~/.Xauthority</span></code> has to be written.  If your home directory quota
(check with <code class="docutils literal"><span class="pre">quota</span></code>) is exceeded, then this can&#8217;t be written and
graphical programs can&#8217;t open.  If your quota is exceeded, clean up
some files, close connections, and log in again.  You can find where
most of your space goes with <code class="docutils literal"><span class="pre">du</span> <span class="pre">-h</span> <span class="pre">$HOME</span> <span class="pre">|</span> <span class="pre">sort</span> <span class="pre">-hr</span> <span class="pre">|</span> <span class="pre">less</span></code>.</p>
<p>This is often the case if you get <code class="docutils literal"><span class="pre">X11</span> <span class="pre">connection</span> <span class="pre">rejected</span> <span class="pre">because</span> <span class="pre">of</span>
<span class="pre">wrong</span> <span class="pre">authentication</span></code>.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="general.html" class="btn btn-neutral float-right" title="Running programs on Triton" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dgx.html" class="btn btn-neutral" title="Nvidia DGX machines" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Aalto Science-IT.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/redirect-to-https.js"></script>
      <script type="text/javascript" src="https://users.aalto.fi/~darstr1/minipres-stable.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>